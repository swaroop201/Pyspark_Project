# -*- coding: utf-8 -*-
"""Sparkassignment

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rDre2jzMcGUKpsZ2wy0AFhUFDEguM6Fe
"""



pip install pyspark

from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql import SQLContext

from pyspark.sql import SparkSession
database = "Coviddata" #your database name
collection = "cases" #your collection name
connectionString=   ('mongodb+srv://Revanth:cheeku@cluster0.l7yfihd.mongodb.net/?retryWrites=true&w=majority')
spark = SparkSession\
.builder\
    .config('spark.mongodb.input.uri',connectionString)\
    .config('spark.mongodb.output.uri', connectionString)\
    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1')\
.getOrCreate()\
# Reading from MongoDB
cases = spark.read\
.format("com.mongodb.spark.sql.DefaultSource")\
.option("uri", connectionString)\
.option("database", database)\
.option("collection", collection)\
.load()
cases.show()

from pyspark.sql import SparkSession
database = "Coviddata" #your database name
collection = "region" #your collection name
connectionString=   ('mongodb+srv://Revanth:cheeku@cluster0.l7yfihd.mongodb.net/?retryWrites=true&w=majority')
spark = SparkSession\
.builder\
    .config('spark.mongodb.input.uri',connectionString)\
    .config('spark.mongodb.output.uri', connectionString)\
    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1')\
.getOrCreate()\
# Reading from MongoDB
region = spark.read\
.format("com.mongodb.spark.sql.DefaultSource")\
.option("uri", connectionString)\
.option("database", database)\
.option("collection", collection)\
.load()
region.show()

from pyspark.sql import SparkSession
database = "Coviddata" #your database name
collection = "time-province" #your collection name
connectionString=   ('mongodb+srv://Revanth:cheeku@cluster0.l7yfihd.mongodb.net/?retryWrites=true&w=majority')
spark = SparkSession\
.builder\
    .config('spark.mongodb.input.uri',connectionString)\
    .config('spark.mongodb.output.uri', connectionString)\
    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1')\
.getOrCreate()\
# Reading from MongoDB
timeprovince = spark.read\
.format("com.mongodb.spark.sql.DefaultSource")\
.option("uri", connectionString)\
.option("database", database)\
.option("collection", collection)\
.load()
timeprovince.show()

casecount=cases.count()
regioncount=region.count()
timeprovincecount=timeprovince.count()
print(casecount)
print(regioncount)
print(timeprovincecount)

cases.describe().show()

distrecords=cases.distinct()
print(distrecords.count())

cases.limit(10).show(10)

cases.withColumnRenamed('case_id','caseid').show()

cf=cases[cases['confirmed']<10].show()

droppedbelow2=cases.na.drop(how='any',subset=['confirmed'])
droppedbelow2.show()

cases.filter((cases.confirmed>100) & (cases.province=='Seoul')).show()

cases.filter((cases.confirmed>10) & (cases.province=='Daegu')).show()

cases.sort('confirmed').show()

from pyspark.sql.functions import col, asc,desc
cases.orderBy(col('confirmed').desc()).show()

from pyspark.sql.functions import sum,avg,max,count

cases.groupby(['province','city']).agg(sum('confirmed')).show()

cases.join(region,cases.city==region.city,how='inner').show()

from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql import SQLContext
cases.registerTempTable('cases_table')
newDF = spark.sql('select * from cases_table where confirmed>100')
newDF.show()

newd1=spark.sql("select * from cases_table where province='Seoul' ").show()

from pyspark.sql.functions import col, udf
from pyspark.sql.types import StringType

convertudf=udf(lambda x:"high" if x>50 else 'Low', StringType())
cases.withColumn('lowhighindicator',convertudf(col('confirmed'))).show()

cases.select(casehighlow(cases.confirmed)).show()